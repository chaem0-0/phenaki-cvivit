{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b154a16",
   "metadata": {},
   "source": [
    "# CViViT forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d294bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from phenaki_pytorch.cvivit import CViViT\n",
    "from phenaki_pytorch.cvivit_trainer import CViViTTrainer\n",
    "\n",
    "cvivit = CViViT(\n",
    "    dim=512,  # embedding size\n",
    "    codebook_size=8192,  # codebook size\n",
    "    image_size=128,  # H,W\n",
    "    patch_size=8,  # spatial patch size\n",
    "    local_vgg=True,\n",
    "    wandb_mode='disabled',\n",
    "    temporal_patch_size=2,  # temporal patch size\n",
    "    spatial_depth=4,  # nb of layers in the spatial transfo\n",
    "    temporal_depth=4,  # nb of layers in the temporal transfo\n",
    "    dim_head=64,  # hidden size in transfo\n",
    "    heads=8,  # nb of heads for multi head transfo\n",
    "    ff_mult=4,  # 32 * 64 = 2048 MLP size in transfo out\n",
    "    commit_loss_w=1.,  # commit loss weight\n",
    "    gen_loss_w=1.,  # generator loss weight\n",
    "    perceptual_loss_w=1.,  # vgg loss weight\n",
    "    i3d_loss_w=1.,  # i3d loss weight\n",
    "    recon_loss_w=10.,  # reconstruction loss weight\n",
    "    use_discr=0,  # whether to use a stylegan loss or not\n",
    "    gp_weight=10\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder='.',\n",
    "    batch_size=1,\n",
    "    force_cpu=False,\n",
    "    wandb_mode='disabled',\n",
    "    train_on_images=False,\n",
    "    grad_accum_every=4,  # use this as a multiplier of the batch size\n",
    "    # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "    use_ema=False,\n",
    "    num_train_steps=100000,\n",
    "    lr=0.0001,  # Learning rate\n",
    "    wd=0.0001,  # Weight decay\n",
    "    max_grad_norm=10,  # gradient clipping\n",
    "    # start the warmup at this factor of the lr\n",
    "    linear_warmup_start_factor=0.5,\n",
    "    # nb of iterations for the warm up\n",
    "    linear_warmup_total_iters=10000,\n",
    "    # nb of iterations for the cosine annealing\n",
    "    cosine_annealing_T_max=100000,\n",
    "    cosine_annealing_eta_min=0.00005,  # lr at the end of annealing\n",
    "    results_folder='results/',\n",
    "    inference=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.load('CVIVIT/')\n",
    "trainer.vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff409475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "real_frames = []\n",
    "\n",
    "for i in range(len(os.listdir('example_cvivit/obvious/'))):\n",
    "    video = cv2.VideoCapture('example_cvivit/obvious/obvious____split_'+str(i+1)+'.mp4')\n",
    "    frames = []\n",
    "\n",
    "    transform = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "\n",
    "    check = True\n",
    "    while check:\n",
    "        check, frame = video.read()\n",
    "\n",
    "        if not check:\n",
    "            continue\n",
    "\n",
    "        # if exists(crop_size):\n",
    "        #    frame = crop_center(frame, *pair(crop_size))\n",
    "\n",
    "        frames.append(rearrange(frame, '... -> 1 ...'))\n",
    "\n",
    "    # convert list of frames to numpy array\n",
    "    frames = np.array(np.concatenate(frames, axis=0))\n",
    "    frames = rearrange(frames, 'f h w c -> c f h w')\n",
    "\n",
    "    def bgr_to_rgb(video_tensor):\n",
    "        video_tensor = video_tensor[[2, 1, 0], :, :, :]\n",
    "        return video_tensor\n",
    "\n",
    "    frames_torch = bgr_to_rgb(frames)\n",
    "\n",
    "    frames_torch = torch.tensor(frames_torch).float()/255.\n",
    "\n",
    "    frames_torch = rearrange(frames_torch, 'c f h w -> 1 c f h w')\n",
    "    \n",
    "    real_frames.append(frames_torch[0])\n",
    "    \n",
    "    codebook_ids = trainer.vae(frames_torch.cuda(), return_only_codebook_ids = True)\n",
    "    \n",
    "    recons = trainer.vae.decode_from_codebook_indices(codebook_ids)\n",
    "    \n",
    "    if i == 0:\n",
    "        final = recons\n",
    "    else:\n",
    "        final = torch.vstack((final, recons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f99b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phenaki_pytorch.data import video_tensor_to_gif\n",
    "from IPython.display import display, Image\n",
    "for i, tensor in enumerate(final.unbind(dim = 0)):\n",
    "    \n",
    "    print('real video:')\n",
    "    video_tensor_to_gif(real_frames[i].cpu(), 'original_video_'+str(i)+'.gif')\n",
    "    display(Image('original_video_'+str(i)+'.gif'))\n",
    "    \n",
    "    print('reconstruction:')\n",
    "    video_tensor_to_gif(tensor.cpu(), 'reconstructed_video_'+str(i)+'.gif')\n",
    "    display(Image('reconstructed_video_'+str(i)+'.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd6739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
